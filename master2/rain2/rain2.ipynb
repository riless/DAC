{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Challenge Kaggle: How Much Did It Rain? II\n",
    "  - Etudiant: Ghilas BELHADJ\n",
    "  - Challenge URL: https://www.kaggle.com/c/how-much-did-it-rain-ii\n",
    "\n",
    "## Description\n",
    "Il s'agit dans ce challenge de prédire la quantité d'eau de pluie (en mm) recueillit dans des jauges prevu à cet effet sur un interval de temps d'une heure. Pour celà nous disposons de données concernant des précipitations antérieurs. Nous allons dans ce qui suit, essayer d'apprendre un modèle qui puisse faire cette prédiction là.\n",
    "\n",
    "## Outils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import cPickle as cpk\n",
    "\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn import cross_validation\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Données du challenge\n",
    "[Les données du challenge](https://www.kaggle.com/c/how-much-did-it-rain-ii/data) doivent se trouver dans le dossier `data` afin de pouvoir executer ce IPython Noteook.\n",
    "```\n",
    "data\n",
    "   ├── test.csv\n",
    "   └── train.csv\n",
    "```\n",
    "\n",
    "Les données sont sous formes de séquences groupées par heure, les entrées qui appartiennent à la même heure ont un identifiant commun et unique. Ensuite chaque entrées comprend differentes mesures prise par un radar à un instant donné.\n",
    "\n",
    "\n",
    "## Description des attributs\n",
    "On vas se reférer à la [description donnée sur Kaggle](https://www.kaggle.com/c/how-much-did-it-rain-ii/data) pour voir à quoi correspond chaque attribut.\n",
    "\n",
    "  * Id: Un identifiant unique d'un ensemble d'observations sur une heure\n",
    "  * minutes_past: La minute à laquelle l'observation a été faite ( relative a l'heure d'observation de chaque ensemble d'observations )\n",
    "  * radardist_km: Dsitance de la jauge par rapport au radar qui prend la mesure sur cette gauge.\n",
    "  * Ref: [Réflectivité](https://fr.wikipedia.org/wiki/R%C3%A9flectivit%C3%A9) en km ( c'est un rapport entre l'énergie réfléchie par rapport à l'énergie incidente d'un objet)\n",
    "  * Ref_5x5_10th: 10émè [centile](https://fr.wikipedia.org/wiki/Centile) de reflexivité dans un voisinage 5x5 autour de la gauge.\n",
    "  * Ref_5x5_50th: 50ème centile.\n",
    "  * Ref_5x5_90th: 90ème centile.\n",
    "  * RefComposite: Reflexivité maximum sur la colonne vertical au dessus de la gauge ( en [décibel Z](https://fr.wikipedia.org/wiki/D%C3%A9cibel_Z) )\n",
    "  * RefComposite_5x5_10th: 10ème centile.\n",
    "  * RefComposite_5x5_50th: 50ème centile.\n",
    "  * RefComposite_5x5_90th: 90ème centile.\n",
    "  * RhoHV: Un coeficient de correlation (sans unité)\n",
    "  * RhoHV_5x5_10th: 10ème centile.\n",
    "  * RhoHV_5x5_50th: 50ème centile.\n",
    "  * RhoHV_5x5_90th: 90ème centile.\n",
    "  * Zdr: Reflectivité différentiel en [Décibel](https://fr.wikipedia.org/wiki/D%C3%A9cibel)\n",
    "  * Zdr_5x5_10th: 10ème centile.\n",
    "  * Zdr_5x5_50th: 50ème centile.\n",
    "  * Zdr_5x5_90th: 90ème centile.\n",
    "  * Kdp: La phase différentielle spécifique (Utilisé pour localiser les régions à forte précipitations/Attenuations)\n",
    "  * Kdp_5x5_10th: 10ème centile.\n",
    "  * Kdp_5x5_50th: 50ème centile.\n",
    "  * Kdp_5x5_90th: 90ème centile.\n",
    "  * Expected: L'observation actuelle de l'etat de la gauge (en mm)\n",
    "\n",
    "\n",
    "## Aperçu des données\n",
    "#### Lecture des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_path = \"data/\"\n",
    "train_file= \"train.csv\"\n",
    "test_file= \"test.csv\"\n",
    "\n",
    "print \"Reading %s\" % input_file\n",
    "time0 = dt.datetime.now()\n",
    "\n",
    "df_train = pd.read_csv(data_path+train_file)\n",
    "df_train.set_index('Id')\n",
    "\n",
    "# test_file = pd.read_csv(data_path+train_file)\n",
    "test_file.set_index('Id')\n",
    "\n",
    "time1 = dt.datetime.now()\n",
    "print('Read in %i sec' % (time1-time0).seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Analyse des données\n",
    "Colonne | max | min | std | mean\n",
    "---|---|---|---|---\n",
    "minutes_past |  59 | 0 | 17.30 | 29.52\n",
    "radardist_km |  21 | 0 | 4.20 | 11.06\n",
    "Ref |  71 | -31 | 10.35 | 22.92\n",
    "Ref_5x5_10th |  62.5 | -32 | 9.20 | 19.95\n",
    "Ref_5x5_50th |  69 | -32 | 10.05 | 22.61\n",
    "Ref_5x5_90th |  72.5 | -28.5 | 11.10 | 25.89\n",
    "RefComposite |  92.5 | -32 | 10.68 | 24.71\n",
    "RefComposite_5x5_10th | 66 | -31 | 9.70 | 22.15\n",
    "RefComposite_5x5_50th | 71 | -27.5 | 10.42 | 24.42\n",
    "RefComposite_5x5_90th | 93.5 | -25 | 11.49 | 27.36\n",
    "RhoHV | 1.05 | 0.2 | 0.09 | 0.97\n",
    "RhoHV_5x5_10th | 1.05 | 0.2 | 0.12 | 0.91\n",
    "RhoHV_5x5_50th | 1.05 | 0.2 | 0.07 | 0.97\n",
    "RhoHV_5x5_90th | 1.05 | 0.2 | 0.04 | 1.01\n",
    "Zdr |   7.93 | -7.87 | 1.51 | 0.53\n",
    "Zdr_5x5_10th |   7.93 | -7.87 | 1.00 | -0.71\n",
    "Zdr_5x5_50th |   7.93 | -7.87 | 0.93 | 0.33\n",
    "Zdr_5x5_90th |   7.93 | -7.87 | 1.67 | 2.07\n",
    "Kdp | 179 | -96.04 | 3.86 | 0.03\n",
    "Kdp_5x5_10th |   3.51 | -80.79 | 2.79 | -3.48\n",
    "Kdp_5x5_50th |  12.8 | -78.77 | 2.26 | -0.47\n",
    "Kdp_5x5_90th | 144.6 | -100.2 | 4.14 | 4.07\n",
    "Expected |   33017.73 | 0.01 | 548.60 | 108.62\n",
    "\n",
    "Une fois que nous avont pris connaissance de chaque attribut et de sa plage de valeur, nous pouvons nettoyer les données des outlayers.\n",
    "\n",
    "Les attributs dont le max et le min ne permettent pas de retrouver le mean, contienent surrement des outliers.\n",
    "\n",
    "**Exemple:** *Expected* contient une valeur maximum de 33017.73 ... improbable. Les points qui ont des valeurs plus grande que 102mm/h sont probablement des outliers. (source: http://thewatchers.adorraeli.com/2015/07/15/record-rainfall-freak-storm-dumps-102-mm-of-rain-in-1-hour-norway/)\n",
    "\n",
    "![Distro de chaque attribut](img/dist.png \"Distro de chaque attribut\")\n",
    "\n",
    "## Que faire des valeurs nulles\n",
    "Certaines données ne sont pas disponible pour une raison ou pour une autre, il est ici question de décider si l'ont doit éliminer les lignes dans lesquelles elle se trouvent, ou bien les déduire afin d'utiliser un plus grand nombre de données.\n",
    "\n",
    "Les valeurs suivantes représente le nombre de NaN sur chaque colonne:\n",
    "```\n",
    "colonne id, 0 NaN\n",
    "colonne min, 0 NaN\n",
    "colonne km, 0 NaN\n",
    "colonne ref, 6533728 NaN\n",
    "colonne ref10, 7573458 NaN\n",
    "colonne ref50, 6524131 NaN\n",
    "colonne ref90, 5368429 NaN\n",
    "colonne refc, 6176380 NaN\n",
    "colonne refc10, 7110407 NaN\n",
    "colonne refc50, 6178189 NaN\n",
    "colonne refc90, 5102588 NaN\n",
    "colonne rho, 7928015 NaN\n",
    "colonne rho10, 8709031 NaN\n",
    "colonne rho50, 7924578 NaN\n",
    "colonne rho90, 6986970 NaN\n",
    "colonne zdr, 7928015 NaN\n",
    "colonne zdr10, 8709031 NaN\n",
    "colonne zdr50, 7924578 NaN\n",
    "colonne zdr90, 6986970 NaN\n",
    "colonne kdp, 8666078 NaN\n",
    "colonne kdp10, 9404373 NaN\n",
    "colonne kdp50, 8660920 NaN\n",
    "colonne kdp90, 7815206 NaN\n",
    "colonne exp, 0 NaN\n",
    "```\n",
    "\n",
    "Sur les 12 786 237 lignes du fichier ̀̀ train.csv` environs ~7 000 000 valeurs sont manquantes sur chaque colonne. Il s'agit d'un très grand nombre de lignes, et des stratégies doivent être utilisé pour combler ce manque d'information.\n",
    "\n",
    "Si l'on enlève chaque ligne pour peu qu'elle contienne une valeur nulle, il nous restera 2 735 878 lignes pour l'apprentissage, contre 717 625 à prédir (test.csv).\n",
    "\n",
    "De plus, on n'as pas le droit de retirer des lignes depuis le fichier `test.csv` car on doit soumettre impérativement 717 625 lignes sur Kaggle pour ce challenge, ce qui implique que l'ont doit remplir les NaN du fichier test.csv pour pouvoir faire de la prédiction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Netoyyage des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "data_path = \"data/\"\n",
    "\n",
    "input_files=[ \"train.csv\", \"test.csv\" ] # 13765202 lines\n",
    "\n",
    "for input_file in input_files:\n",
    "    print \"Reading %s\" % input_file\n",
    "    time0 = dt.datetime.now()\n",
    "    df = pd.read_csv(data_path+input_files)\n",
    "    time1 = dt.datetime.now()\n",
    "    print('Read in %i sec' % (time1-time0).seconds)\n",
    "\n",
    "    print \"\"\"Drop minutes_past columns\"\"\"\n",
    "    df = df.drop('minutes_past', axis=1)\n",
    "\n",
    "    print \"\"\"Create sample_weights feature\"\"\"\n",
    "    max_km = df[\"radardist_km\"].max()\n",
    "    df['sample_weights'] = df['radardist_km'].apply( lambda x:1-x/max_km )\n",
    "\n",
    "    print \"\"\"Drop radardist_km columns\"\"\"\n",
    "    df = df.drop('radardist_km', axis=1)\n",
    "\n",
    "    print \"\"\"Create missing_values feature\"\"\"\n",
    "    nb_extra_cols = 1\n",
    "    if ( input_file == \"train.csv\"):\n",
    "        nb_extra_cols = 2\n",
    "    nb_columns = len(df.columns) - nb_extra_cols # without Expected and Id\n",
    "    df['missing_values'] = df.isnull().sum(axis=1) / nb_columns\n",
    "\n",
    "    grouped = df.groupby('Id')\n",
    "\n",
    "    value_cols = [ 'Ref','Ref_5x5_10th','Ref_5x5_50th','Ref_5x5_90th',\n",
    "    'RefComposite','RefComposite_5x5_10th','RefComposite_5x5_50th','RefComposite_5x5_90th',\n",
    "    'RhoHV','RhoHV_5x5_10th','RhoHV_5x5_50th','RhoHV_5x5_90th',\n",
    "    'Zdr','Zdr_5x5_10th','Zdr_5x5_50th','Zdr_5x5_90th',\n",
    "    'Kdp','Kdp_5x5_10th','Kdp_5x5_50th','Kdp_5x5_90th' ]\n",
    "\n",
    "    agg_cols = lambda col: {col+'_max' : np.max, col+'_median' : np.median}\n",
    "\n",
    "    print \"\"\"Agg by mean\"\"\"\n",
    "    agg_df = grouped.agg(np.mean) \n",
    "\n",
    "    print \"\"\"Create median, max aggregations\"\"\" \n",
    "    for col in value_cols:\n",
    "        agg_df = pd.concat( [agg_df, grouped[col].agg(agg_cols(col))], axis=1 )\n",
    "\n",
    "\n",
    "    print \"\"\"Fill left NaN with median\"\"\"\n",
    "    agg_df = agg_df.fillna(agg_df.median())\n",
    "\n",
    "\n",
    "    if ( input_file == \"train.csv\"):\n",
    "        print \"\"\"Clean Expected column\"\"\"\n",
    "        agg_df = agg_df[agg_df['Expected'] <= 100]\n",
    "\n",
    "\n",
    "    clean_name = \"clean_\"+input_file\n",
    "    print \"\"\"Saving %s\\n\\n\"\"\" % clean_name\n",
    "    agg_df.to_csv(data_path+clean_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrainement des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import cPickle as cpk\n",
    "\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn import cross_validation\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "cleantrain_file=\"data/clean_train.csv\" # 12786237 lines\n",
    "\n",
    "print \"Lecture clean csv...\"\n",
    "time0 = dt.datetime.now()\n",
    "\n",
    "cols = [ 'Id', 'Expected', 'missing_values',\n",
    "'Ref_5x5_10th_max','Ref_5x5_50th_max','sample_weights',\n",
    "'Ref_5x5_90th','RefComposite_5x5_90th','Ref_5x5_90th_max',\n",
    "'RefComposite_5x5_90th_median','RefComposite_5x5_90th_max',\n",
    "'Zdr_5x5_90th','Zdr_5x5_90th_max','Ref_5x5_90th_median','Zdr_5x5_90th_median']\n",
    "\n",
    "df = pd.read_csv(cleantrain_file, usecols=cols, nrows=100000)\n",
    "\n",
    "time1 = dt.datetime.now()\n",
    "print('lecture en: %i sec' % (time1-time0).seconds)\n",
    "print \"Nb lignes: %d \",  df.shape[0]\n",
    "\n",
    "print \"Prepare data, labels\"\n",
    "data = df.as_matrix()\n",
    "label = df[['Expected']].as_matrix().ravel()\n",
    "df = df.drop(['Expected', 'Id'], axis=1)\n",
    "\n",
    "# print \"Prepare folds for cross validation\"\n",
    "x_train, x_test, y_train, y_test = cross_validation.train_test_split(data, label, test_size=0.8, random_state=23435)\n",
    "\n",
    "\n",
    "# conf = sklearn.metrics.confusion_matrix(df['missing_values'], df['sample_weights'])\n",
    "# plt.imshow(conf, cmap='binary', interploation='None')\n",
    "\n",
    "print \"RandomForestRegressor...\"\n",
    "clf = sklearn.ensemble.RandomForestRegressor(verbose=2, n_jobs=2)\n",
    "clf.fit(x_train, y_train)\n",
    "print mean_squared_error( clf.predict(x_test), y_test )\n",
    "# with open('models/RandomForestRegressor.pkl', 'wb') as fid:\n",
    "#     cpk.dump(clf, fid)\n",
    "\n",
    "print \"GradientBoostingRegressor...\"\n",
    "clf = sklearn.ensemble.GradientBoostingRegressor(verbose=2)\n",
    "clf.fit(x_train, y_train)\n",
    "print mean_squared_error( clf.predict(x_test), y_test )\n",
    "# with open('models/GradientBoostingRegressor.pkl', 'wb') as fid:\n",
    "#     cpk.dump(clf, fid)\n",
    "\n",
    "print \"ExtraTreesRegressor...\"\n",
    "\n",
    "\n",
    "clf = ExtraTreesRegressor(n_estimators=20, verbose=2, n_jobs=-1)\n",
    "clf.fit(x_train, y_train)\n",
    "print mean_squared_error( clf.predict(x_test), y_test )\n",
    "# with open('models/ExtraTreesRegressor.pkl', 'wb') as fid:\n",
    "#     cpk.dump(clf, fid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Création de la submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle as cpk\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "\n",
    "model_path = \"models/\"\n",
    "data_path = \"data/\"\n",
    "cleantest_file=\"clean_test.csv\"\n",
    "models_files = ['GradientBoostingRegressor.pkl', 'RandomForestRegressor.pkl', 'ExtraTreesRegressor.pkl']\n",
    "\n",
    "\n",
    "cols = [ 'Id', 'missing_values',\n",
    "'Ref_5x5_10th_max','Ref_5x5_50th_max','sample_weights',\n",
    "'Ref_5x5_90th','RefComposite_5x5_90th','Ref_5x5_90th_max',\n",
    "'RefComposite_5x5_90th_median','RefComposite_5x5_90th_max',\n",
    "'Zdr_5x5_90th','Zdr_5x5_90th_max','Ref_5x5_90th_median','Zdr_5x5_90th_median']\n",
    "\n",
    "time0 = dt.datetime.now()\n",
    "df = pd.read_csv(data_path+cleantest_file, usecols=cols)\n",
    "time1 = dt.datetime.now()\n",
    "print('lecture en: %i sec' % (time1-time0).seconds)\n",
    "print \"Nb lignes: %d \",  df.shape[0]\n",
    "\n",
    "print \"preparing data\"\n",
    "ids = df['Id']\n",
    "df = df.drop(['Id'], axis=1)\n",
    "data = df.as_matrix()\n",
    "\n",
    "\n",
    "pred_exp = [pd.read_csv(\"data/marshall-palmer.csv\")]\n",
    "\n",
    "for model_file in models_files:\n",
    "    with open(model_path+model_file, 'rb') as fid:\n",
    "        print \"Load classifier %s\" % model_file\n",
    "        clf = cpk.load(fid)\n",
    "\n",
    "        print \"Prediction...\"\n",
    "        pred = clf.predict(data)\n",
    "\n",
    "        print \"Saving csv...\"\n",
    "        df_pred = pd.DataFrame({'Expected': pred, 'Id': ids})\n",
    "        pred_exp.append( pred )\n",
    "\n",
    "        df_pred.to_csv(data_path+model_file+'_sol.csv', index=False)\n",
    "\n",
    "sol = pd.DataFrame({'Id': ids, 'Expected': np.mean( pred_exp, axis=0) }) \n",
    "solution_file=\"solution.csv\"\n",
    "sol.to_csv(data_path+solution_file, header=True, cols=[\"Id\",\"Expected\"], index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Résultats\n",
    "\n",
    "    * Public  : 24.06898\n",
    "    * Private : 25.09764"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
