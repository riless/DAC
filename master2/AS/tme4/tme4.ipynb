{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TME4 - Les réseaux profonds avec auto-encodeurs\n",
    "\n",
    "Les autoencodeurs sont de nouveaux types de réseaux de neurones, entrainés afin de reproduire les donnée en entrée $x$ dans les sorties.\n",
    "\n",
    "Nous allons tenter dans ce TME, de créer un autoencodeur avec $5$ couches cachées sur le dataset $MNIST$. et d'apprendre chaque couche avec notre fonction de descente de gradient classique.\n",
    "\n",
    "## Impotation des données MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "  data : ByteTensor - size: 60000x28x28\n",
       "  size : 60000\n",
       "  label : ByteTensor - size: 60000\n",
       "}\n",
       "{\n",
       "  data : ByteTensor - size: 10000x28x28\n",
       "  size : 10000\n",
       "  label : ByteTensor - size: 10000\n",
       "}\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "require 'torch'\n",
    "require 'nn'\n",
    "require 'optim'\n",
    "local mnist = require 'mnist'\n",
    "\n",
    "\n",
    "trainset = mnist.traindataset()\n",
    "testset = mnist.testdataset()\n",
    "\n",
    "print ( trainset )\n",
    "print ( testset )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous disposons de 60000 images de taille $28\\times 28$ et de leurs labels allant de $0..9$ pour l'ensemble d'entrainement ( 10000 pour l'ensemble de test )\n",
    "\n",
    "On vas visualiser la première entrée et son label pour voir a quoi ça ressemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "itorch.image(trainset.data[1])\n",
    "print (trainset.label[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création du MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layer_size = 49\n",
    "input_size = 28*28\n",
    "model = nn.Sequential()\n",
    "\n",
    "model:add(nn.Reshape(input_size))\n",
    "model:add(nn.Linear(input_size, layer_size))\n",
    "model:add(nn.Tanh())\n",
    "model:add(nn.Linear(layer_size, input_size))\n",
    "model:add(nn.Reshape(input_size))\n",
    "\n",
    "criterion = nn.MSECriterion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descente de gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function gradient_descent(model, inputs, labels, criterion)\n",
    "   learning_rate = 1e-3\n",
    "   nb_iter = 1000\n",
    "    \n",
    "   for i = 1,nb_iter do\n",
    "      model:zeroGradParameters()\n",
    "      \n",
    "      idx = torch.randperm(inputs:size(1))\n",
    "        for j = 1, inputs:size(1) do\n",
    "            local x = inputs[idx[j]]:double()\n",
    "            local y = labels[idx[j]]:double()\n",
    "            \n",
    "            local y_hat = model:forward(x)\n",
    "            local loss = criterion:forward(y_hat,y)\n",
    "            local delta = criterion:backward(y_hat,y)\n",
    "            model:backward(x, delta)\n",
    "\n",
    "            -- update\n",
    "            model:updateParameters(learning_rate)\n",
    "        if ( i % 100 == 0) then\n",
    "            print(i,loss)\n",
    "        end\n",
    "      end\n",
    "   end\n",
    "    \n",
    "   return loss\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 60000\n",
       "    28\n",
       "    28\n",
       "[torch.LongStorage of size 3]\n",
       "\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function step(inputs)\n",
    "    local loss = gradient_descent(model, inputs, inputs, criterion)\n",
    "    return loss\n",
    "end\n",
    "\n",
    "function eval(trainset)\n",
    "    local loss = criterion:forward(model:forward(trainset), inputs) -- compare prediction to input\n",
    "    return loss\n",
    "end\n",
    "\n",
    "max_iters = 30\n",
    "\n",
    "for i = 1,max_iters do\n",
    "\n",
    "    local loss = step(trainset.data)\n",
    "    local validation_loss = eval({testset.data})\n",
    "\n",
    "    print(string.format('Iteration: %d loss: %4f | validation loss: %4f', i, loss, validation_loss))\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forme de l'autoencodeur avec 5 couches cachées\n",
    "\n",
    "Nous avons sur chaque couche "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "--[[\n",
    "function auto_encoder_train(encoder,decoder,x, criterion)\n",
    "  auto_encoder = nn.Sequential()\n",
    "    \n",
    "  auto_encoder:add(encoder)\n",
    "  auto_encoder:add(nn.Tanh())\n",
    "  auto_encoder:add(decoder)\n",
    "  auto_encoder:add(nn.Tanh()) \n",
    "\n",
    "  gradient_descent(auto_encoder, x, x, criterion) -- On apprend à reproduire les entrées.\n",
    "  return encoder\n",
    "end\n",
    "\n",
    "\n",
    "layer_sizes = {50, 50, 50, 50}\n",
    "criterion = nn.MSECriterion()\n",
    "ml_encoder = nn.Sequential()\n",
    "\n",
    "\n",
    "for i=1,(#layer_sizes)-1 do:\n",
    "   x = ml_encoder:forward(train_data)\n",
    "    \n",
    "   encoder = nn.Linear(layer_sizes[i],layer_sizes[i+1])\n",
    "   decoder = nn.Linear(layer_sizes[i+1],layer_sizes[i])\n",
    "    \n",
    "   trained_encoder = auto_encoder_train(encoder,decoder,x,criterion)\n",
    "\n",
    "   ml_encoder:add(trained_encoder)\n",
    "   ml_encoder:add(nn.Tanh())\n",
    "end \n",
    "]]--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation des features apprises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "--[[ Autoencodeur: Eviter le vanishing gradient\n",
    "\n",
    "permet de donner la garantie de ne pas perdre de l'information quand je passe d'une couche a l'autre\n",
    "\n",
    "#vacances dans deux semaines\n",
    "à la fin des vacances : rapport AS sur la base MNIST \n",
    "comparaison des deux manière d'ataqu \n",
    "visualisation\n",
    "\n",
    "\n",
    "taille_couches_caches = 5\n",
    "calcule de théta1*\n",
    "]]--\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20004"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
